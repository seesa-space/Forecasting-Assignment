{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Multivariate Time Series Analysis\n", "\n", "## Analysis of Power Consumption Data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["KATE expects your code to define variables with specific names that correspond to certain things we are interested in.\n", "\n", "KATE will run your notebook from top to bottom and check the latest value of those variables, so make sure you don't overwrite them.\n", "\n", "* Remember to uncomment the line assigning the variable to your answer and don't change the variable or function names.\n* Use copies of the original or previous DataFrames to make sure you do not overwrite them by mistake.\n", "\n", "You will find instructions below about how to define each variable.\n", "\n", "Once you're happy with your code, upload your notebook to KATE to check your feedback."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Time series containing historical data for a single variable are known as univariate time series. When multiple variables are involved to predict an outcome over time, this is referred to as multivariate time series. In this case, we use the historical data from multiple different variables in order to generate a forecast.\n", "\n", "In this practical, we consider the analysis of power consumption data."]}, {"cell_type": "code", "execution_count": null, "metadata": {"inputHidden": false, "outputHidden": false}, "outputs": [], "source": ["import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "\n", "from statsmodels.tsa.api import VAR\n", "from statsmodels.tsa.stattools import adfuller\n", "from statsmodels.tsa.seasonal import seasonal_decompose\n", "from sklearn.metrics import mean_absolute_error"]}, {"cell_type": "markdown", "metadata": {}, "source": ["First, we will load the dataset from `data/household_power_consumption.csv` into a DataFrame. The dataset is available [here](https://archive.ics.uci.edu/ml/datasets/individual+household+electric+power+consumption).\n", "\n", "The parameter settings parsed are:\n", "\n", " - `index_col = 'dt'` Set the new datetime format column as the index\n", " - `na_values=['nan','?']` Allows any occurences of 'nan' or '?' to be recognised as NaN.\n", " - `parse_dates = [0] ` Specifies the index of the column that has to be parsed as datetime. \n", " - `infer_datetime_format=True` When combined with the parse_dates argument, this infers the format of the datetime strings in an efficient way"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df = pd.read_csv('data/household_power_consumption.csv',low_memory=False, \n", "                 na_values=['nan','?'], index_col='dt', parse_dates=[0], infer_datetime_format=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.head()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["This dataset contains multivariate time series data describeing the electricity consumption for a single household over four years. It spans December 2006 to November 2010, with the data recorded at minute intervals.\n", "\n", "There are seven variables in this time series, being:\n", "\n", "    global_active_power: The total active power consumed by the household (kilowatts).\n", "    global_reactive_power: The total reactive power consumed by the household (kilowatts).\n", "    voltage: Average voltage (volts).\n", "    global_intensity: Average current intensity (amps).\n", "    sub_metering_1: Active energy for kitchen (watt-hours of active energy).\n", "    sub_metering_2: Active energy for laundry (watt-hours of active energy).\n", "    sub_metering_3: Active energy for climate control systems (watt-hours of active energy).\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Before beginning any analysis we need to explore the data and ensure it is clean and in a suitable format."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's first check the data types to be sure they are in the correct format"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df.dtypes"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Clean the data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**1. How many values are NaNs in each column? Store your answer in a variable called `sum_nans`**\n", "\n", "*Hint: `isna()` will do a Boolean check of if an entry is an NaN or not. Summing these will give you a count.* "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# sum_nans = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**2. Fill the NaNs in `df` by propagating last valid observation forward**\n", "\n", "*Hint: This is one of the possible methods available in the function `fillna()`*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# df = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**3. Make sure that there are no NaNs remaining in each column. Store the sum of NaNs value in each column in a Pandas Series called `sum_nans2`**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# sum_nans2 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Avoid modifying `df` itself in the subsequent questions."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualise the data\n", "\n", "We can now begin to visualise the data and decide on the problem we would like to investigate. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**4. What is the shape of the dataframe `df`?**\n", "\n", "Store your answer in a variable called `df_shape`**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# df_shape = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As the data is recorded at minute intervals, this means we have a very large dataset. With such precise time series data, it can be useful to downsample the activity to daily, weekly or even monthly data. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**5. Create two new dataframes by downsampling `df` to contain daily data.** \n", "\n", "Store in `daily_total` the dataframe with the total amounts for each day, and in `daily_mean` the one with average amount per day.\n", "\n", "*Hint*: Use the `resample()` function specifying 'D' to indicate daily data. You will then need to apply either a summation or a mean function in order to combine the results across each day."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# daily_total = ...\n", "# daily_mean = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**6. Create two new dataframes by downsampling `df` to contain weekly data.** \n", "\n", "Store in `weekly_total` the dataframe with the total amounts for each week, and in `weekly_mean` the one with average amount per week."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# weekly_total = ...\n", "# weekly_mean = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can plot our data using the function defined below."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_dataframe(dataframe):\n", "    '''\n", "    This function takes in a pandas dataframe and plots data for all columns provided in a grid of size n_rowsx2,\n", "    where n_rows is ceil(cols/2) and cols is the number of provided columns in the dataframe.\n", "    '''\n", "    #\u00a0Get the number of columns\n", "    cols = dataframe.shape[1]\n", "    # Create a figure of suitable size\n", "    n_rows = int(np.ceil(cols/2))\n", "    n_cols = 2\n", "    fig, axs = plt.subplots(n_rows, n_cols, figsize = (30, 25));\n", "    # Set the titles to be those of the dataframe variable names \n", "    columns = dataframe.columns\n", "    # In each position, plot the relevant data\n", "    counter = 0\n", "    # If we have an odd number of subplots to make, do the last one separately\n", "    if cols%2 != 0:\n", "        n_rows -= 1\n", "        axs[n_rows, 0].plot(dataframe[columns[-1]]);\n", "        axs[n_rows, 0].set_title(columns[-1], fontweight = 'bold', size = 20);\n", "        fig.delaxes(axs[n_rows, 1])   \n", "    # Do the remaining plots \n", "    for i in range(n_rows):\n", "        for j in range(n_cols):\n", "            axs[i, j].plot(dataframe[columns[counter]]);\n", "            axs[i, j].set_title(columns[counter], fontweight = 'bold', size = 20);\n", "            counter += 1\n", "    # Show the plot\n", "    plt.show()\n", "    return \n", "    "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to plot the first 4 columns of the `daily_mean` using the function `plot_data`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# meandaily_plot = plot_dataframe(daily_mean.iloc[:,0:4])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to produce the same figures for the daily totals"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# totaldaily_plot = plot_dataframe(daily_total.iloc[:,0:4])\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As expected, the overall shape is extremely similar when considering the daily means or the daily totals. We can repeat this for the weekly data and present the mean values of the left and total values on the right. To do this, we define the below function `plot_twodfs`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_twodfs(dataframe1, dataframe2):\n", "    \"\"\"\n", "    A function which plots the data from each dataframe side by side.\n", "    This is such that column 0 of each dataframe will be side by side in row 0 of a 2x2 figure, and so on for all columns. \n", "    NOTE: Therefore dataframe1 and dataframe2 must have the same number of columns.\n", "    \"\"\"\n", "    if dataframe1.shape[1] != dataframe2.shape[1]:\n", "        print('Error: dataframe1 and dataframe2 must have the same number of columns')\n", "        return\n", "    \n", "    n_rows = dataframe1.shape[1]\n", "    n_cols = 2\n", "    fig, axs = plt.subplots(n_rows, 2, figsize = (30, 25))\n", "    \n", "    columns1 = dataframe1.columns\n", "    columns2 = dataframe2.columns\n", "    \n", "    for i in range(n_rows):\n", "        axs[i, 0].plot(dataframe1[columns1[i]]);\n", "        axs[i, 0].set_title('Dataframe1: '+ columns1[i], fontweight = 'bold', size = 20);\n", "\n", "        axs[i, 1].plot(dataframe2[columns2[i]]);\n", "        axs[i, 1].set_title('Dataframe2: '+ columns2[i], fontweight = 'bold', size = 20);\n", "    # Show the plot\n", "    plt.show()    \n", "    \n", "    return"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to use the function `plot_twodfs` to plot the first 4 columns of the weekly mean data alongside the first 4 columns of the weekly total data. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# wmt = plot_twodfs(weekly_mean.iloc[:,0:4], weekly_total.iloc[:,0:4])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For this practical, we will try to predict the global active power for different intervals using the daily mean values. \n", "This is one of the multiple analysis that could be carried on this dataset."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Explore the data further"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As part of the exploratory analysis of the dataset we first decompose the the series to find the trend, seasonal and residual components.\n", "\n", "**7. Using the `seasonal_decompose` function, explore the daily mean `Global_active_power` values.** \n", "\n", "Store its trend, seasonal and residual components in the homonymous variables.\n", "\n", "*Hint*: Use the argument `period` to see if there is an annual seasonality component "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# trend = ...\n", "# seasonal = ...\n", "# residual = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We now define the following function which will plot the results of the seasonality decomposition."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def plot_decomposition(original, trend, seasonal, residual):\n", "    \"\"\"\n", "    Plot the components of a decomposition. Pass in the original univarate data which was decomposed, and then the \n", "    3 resulting components, being the trend, seasonal and residual parts. \n", "    \"\"\"\n", "    # Plot the results\n", "    plt.subplot(411)\n", "    plt.plot(original, label = 'Original');\n", "    plt.legend(loc = 'best')\n", "    plt.subplot(412)\n", "    plt.plot(trend, label = 'Trend');\n", "    plt.legend(loc = 'best')\n", "    plt.subplot(413)\n", "    plt.plot(seasonal, label = 'Seasonality');\n", "    plt.legend(loc = 'best')\n", "    plt.subplot(414)\n", "    plt.plot(residual, label = 'Residuals');\n", "    plt.legend(loc = 'best')\n", "    plt.tight_layout()\n", "    plt.show()\n", "    return"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to create a 4-by-1 figure showing the original series(`daily_mean['Global_reactive_power']`) and its `trend`, `seasonal` and `residual` components."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# seasonality_plot = plot_decomposition(daily_mean['Global_reactive_power'], trend, seasonal, residual)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Stationarity for Multivariate Time Series\n", "\n", "Recall that stationarity is an important feature of time series data and that a stationary time series has properties that do not depend on time. In particular, the mean and variance are independent of time.\n", "\n", "As in the univariate case, when looking at multivariate data, we need each column to be a stationary time series in order to be able to apply many common time series models. \n", "\n", "To do this, we use the Augmented Dickey-Fuller test, which is a statistical test for stationarity. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**8. Write a function, `adf_test` which does the following:**\n", "\n", " - Takes in input of a column of a dataframe, `df_col`.\n", "\n", " - Define `dftest` to be the result of applying the function `adfuller()` to `df_col`. \n", "\n", " - Set `p` to store the p-value. This will be contained as one of the entries in the `dftest` tuple.\n", "\n", " - Creates the variable `result` in the following way: if p<=0.05 it contains the string \"column_name is stationary\", otherwise contains the string \"column_name is not stationary\".\n", "\n", " - returns `result`\n", "\n", "One example of an accepted output is the following: 'Global_active_power is stationary'\n", "\n", "*Note*: The Augmented Dickey-Fuller test generates a tuple consisting of 6 parameters: \n", " - the ADF test statistic\n", " - the p-value\n", " - number of lags used\n", " - number of observations used\n", " - critical values at 1%, 5%, 10% levels\n", " - the maximized information criterion (icbest).\n", " \n", "*Hint*: to get the column name you can use `df_col.name`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# def adf_test(df_col):\n", "#     pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to check if the columns of `daily_mean` are stationary."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# for i in range(daily_mean.shape[1]):\n", "#     print(adf_test(daily_mean.iloc[:,i]))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We have been lucky, since all the columns are stationary. If this wasn't the case, we would precede by differencing and redoing the dickey fuller test. This step of checking for stationarity plus differencing might have to be repeated more than once to fully ensure all variables are stationary."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cross correlation \n", "\n", "The final aspect we need to consider before building our model is the correlations amongst features. There are several ways to consider the correlation between time series including Pearson correlation, Spearman correlation, and cross-correlation functions. \n", "\n", "Note that when two time series variables are non-stationary, they may show strong correlation even through the underlying processes have no casual relationships. This strong correlation may be purely caused by the fact that the two time series variables have non-constant mean. This phenomenon is called spurious relationship and is worth considering in the non-stationary case.\n", "\n", "For extra material on quantifying correlation see the link [here](https://towardsdatascience.com/four-ways-to-quantify-synchrony-between-time-series-data-b99136c4a9c9)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**9. Use the `.corr()` function on the `daily_mean` dataframe to get the correlation between the variables. In particular, specify `spearman` as the method to use. Store the output in `daily_corr`.**\n", "\n", "*Note*: Spearman's correlation assesses if relationships are linear or not. A perfect Spearman correlation of +1 or \u22121 occurs when each of the variables is a perfect linear function of the other. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# daily_corr = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can visualize the correlation matrix as a heatmap using the function `sns.heatmap()`. Uncomment the following cell to visualize `daily_corr`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# hm1 = sns.heatmap(daily_corr, \n", "#         xticklabels=daily_mean.columns,\n", "#         yticklabels=daily_mean.columns);\n", "# hm1.set_title('Daily Mean Correlation Matrix');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see from the above heatmap that the daily global active power is very strongly correlated with the global intensity, and strongly correlated with all of the sub metering measurements. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to visualize the correlation heatmap for `weekly_mean`. What do you notice?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# hm2 = sns.heatmap(weekly_mean.corr(method='spearman'), \n", "#         xticklabels=weekly_mean.columns,\n", "#         yticklabels=weekly_mean.columns);\n", "# hm2.set_title('Weekly Mean Correlation Matrix');"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can see that with resampling techniques it is possible to change the correlations among features. This is an important point to keep in mind for feature engineering."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Building the Model: VAR\n", "\n", "\n", "Vector autoregression (VAR) is a very flexible and easy to use model for the analysis of multivariate time series. It is an extension of the univariate AR model. \n", "\n", "In a VAR model, each variable is a linear function of the past values of itself and the past values of all the other variables. \n", "\n", "Consider the case of bivariate data, that is we have 2 time series of interest. Using 1 lag, VAR will model the following:\n", "\n", "<br>\n", "\n", "\n", "```Python\n", "y1(t) = a1 + w11 * y1(t-1) + w12 y2(t-1) + e1(t-1)\n", "```\n", "\n", "and\n", "\n", "```Python\n", "y2(t) = a2 + w21 * y1(t-1) + w22 y2(t-1) + e2(t-1)\n", "```\n", "\n", "Here: \n", "- `a1` and `a2` are the constant terms,\n", "- `w11`, `w12`, `w21`, and `w22` are the coefficients,\n", "- `e1` and `e2` are the error terms\n", "\n", "This can be extended and written in matrix notation for lag up to `p` as: \n", "\n", "```Python\n", "y(t) = a + w1 * y(t-1) + ... + wp * y(t-p) + e(t), \n", "```\n", "\n", "where now, `y` is a vector containing `y1`, `y2`, all the way up to `yp`, and `w1` is a vector containing the values `w11`, `w12`, all the way up to `wp`. Lastly `e` is a vector containing all of the error terms.\n", "\n", "You can find the documentation about the VAR model [here](https://www.statsmodels.org/dev/vector_ar.html)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We will now consider 3 types of forecasting problems as assess how the VAR model performs in each.\n", "\n", "1) Predicting 1 week ahead based on all historical data. This is a fairly short-term forecast.\n", "\n", "2) Predicting 2 months ahead based on all historical data. This is a much longer term forecast. \n", "\n", "3) Multi-step forecast for 1 month. This involves making a prediction for one time step, taking the prediction, and feeding it into the model as an input in order to predict the subsequent time step. This process is repeated until the desired number of steps have been forecasted. In practice this can be very useful, especially in conjunction with other forecasts as it allows us to make full use of the current and updated data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Predicting 1 week ahead."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**10. Create a copy of the `daily_mean` dataframe and store it in `model_ds`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# model_ds = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**11. Create two new DataFrames. The first one, `ds_train1`, with all the rows of `model_ds` except for the last 7. The second one, `ds_test1`, with only the last 7 rows of `model_ds`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# ds_train1 = ...\n", "# ds_test1 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**12. Using the function `VAR`, define the model using `ds_train1`. Then fit via the `.fit()` function and define the result as `model1_fit`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# model1_fit = ...\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# print(model1_fit.summary())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to print the summary of `model1_fit`."]}, {"cell_type": "markdown", "metadata": {}, "source": ["The summary provides us with information about the model fit. We can see the AIC reported and also standard errors and results for the full multivariate model. We are only interested in the results for the equation global active power as this is what we are forecasting. "]}, {"cell_type": "markdown", "metadata": {}, "source": ["**13. Create an array `prediction1` that contains the predictions on the 7 values of `ds_test1`.**   \n", "\n", "You can use the `forecast` function with parameters `model1_fit.endog`, and `steps` as the length for which we wish to forecast.\n", "\n", "*Note*: `model1_fit.endog` contains an np array of `ds_train1` and so you can also use `np.asarray(ds_train1)`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "#\u00a0prediction1 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can notice that the prediction contains a large array of forecasts for each of the columns in our dataset. We are interested in column 0. Therefore we define the below function `transform_prediction` which converts the model forecast result to an array of univariate predictions for the `Global_active_power`. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def transform_prediction(prediction):\n", "    return [prediction[i][0] for i in range(len(prediction))]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Using `transform_prediction` we convert the model forecast to predictions that we can compare to our test data. Uncomment the following cell to plot `predictions_7day` against the actual data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictions_7day = transform_prediction(prediction1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# fig1 = plt.figure()\n", "# plt.plot(ds_test1.index, ds_test1.iloc[:,0], label='Actual');\n", "# plt.plot(ds_test1.index, predictions_7day, label='Forecast');\n", "# plt.xticks(rotation=45);\n", "# plt.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**14. What is the mean absolute error between the test data for global active power and our 7 day prediction?**\n", "\n", "Store your answer in a variable called `MAE1`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# MAE1 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Predicting 2 months ahead. \n", "#### Using the methodology given for the 1 week forecast, we will now predict 2 months ahead."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**15. Create two new DataFrames. The first one, `ds_train2`, with all the rows of `model_ds` except for the last 60. The second one, `ds_test2`, with only the last 60 rows of `model_ds`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# ds_train2 = ...\n", "# ds_test2 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**16. Create a variable named `model2_fit` where you store the `VAR` model fitted on `ds_train2`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# model2_fit = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to print the summary of `model2_fit`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# print(model2_fit.summary())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**17. Create an array called `prediction2` that contains the predictions on the 60 values of `ds_test2`.**  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# prediction2 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to plot `predictions_60day` against the actual data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["predictions_60day = transform_prediction(prediction2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# fig2 = plt.figure()\n", "# plt.plot(ds_test2.index, ds_test2.iloc[:,0], label='Actual');\n", "# plt.plot(ds_test2.index, predictions_60day, label='Forecast');\n", "# plt.xticks(rotation=45);\n", "# plt.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**18. What is the mean absolute error between the test data for global active power and our 60 day prediction?**\n", "\n", "Store your answer in a variable called `MAE2`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# MAE2 = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3) 1 month multi-step forecast.\n", "\n", "As stated previously, multi-step forecasting involves making a prediction for one time step, taking the prediction, and feeding it into the model as an input in order to predict the subsequent time step. \n", "\n", "We repeat this process is until the we have forecasted the desired number of steps.\n", "\n", "To read more about this, visit the article [here](https://machinelearningmastery.com/multi-step-time-series-forecasting-with-machine-learning-models-for-household-electricity-consumption/)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["**19. Create a function `ms_forecast` which implements a multi-step forecast.** \n", "\n", "- The input should be the dataframe, `df`, and the number of steps to forecast, `n_obs`. \n", "\n", "- Begin by setting `preds` to be an empty list.\n", "\n", "- The function should then contain a for loop that does the following:\n", "\n", "    - 1) Loop from i being 0 to `n_obs`\n", "\n", "    - In the loop:\n", "\n", "    - 2) Define `model` to be a VAR model using the multivariate data, excluding the last `n_obs-i` values. \n", "    - Note: In this way, when i is 0, we will exclude the last n_obs days and forecast 1 step, when i is 1, we will exclude only the last n_obs-1 days and forecast another step. And so on until we have all the data except the last value which we forecast.\n", "\n", "    - 3) Fit the model using `.fit()` and assign this to `model_fit`.\n", "\n", "    - 4) Create a prediction using `.forecast` with the `model_fit.endog` and 1 step. \n", "    - Hint: the output of this will give you an array containing an array of 1-step forecasted values for all columns in our dataframe. As we are only interested in the first column, index the output of the forecast with `[0][0]`.\n", "\n", "    - 5) Append the prediction to `preds`\n", "\n", "- The function should return ` preds`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# def ms_forecast(df, n_obs):\n", "#     pass\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**20. Use the function `ms_forecast` on `model_ds`, with `n_obs` = 30. Storing the output in a variable called `preds`.**"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# preds = ...\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Uncomment the following cell to plot the prediction with the actual values."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# fig3 = plt.figure()\n", "# plt.plot(model_ds.iloc[-n_obs:,0].index, model_ds.iloc[-n_obs:,0], label='Actual');\n", "# plt.plot(model_ds.iloc[-n_obs:,0].index, preds, label='Forecast');\n", "# plt.xticks(rotation=45);\n", "# plt.legend();"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**21. What is the mean absolute error between the test data for global active power and our 30 day prediction?**\n", "\n", "Store your answer in a variable called `MAE3`\n", "\n", "*Hint*: you should compute the mean absolute error between vector `preds` and the last 30 rows of the first column of `model_ds`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Add your code below\n", "# MAE3 = ...\n"]}], "metadata": {"kernel_info": {"name": "python3"}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.7.12"}, "nteract": {"version": "0.15.0"}}, "nbformat": 4, "nbformat_minor": 1}